result <- quicksort(v)
print(result)
quicksort <- function(v) {
len <- length(v)
posicion.pivote <- sample(c(1: len), 1)
pivote <- v[posicion.pivote]
cat("\n pivote: ", pivote," en posicion: ",posicion.pivote)
izq <- c()
der <- c()
if (posicion.pivote > 1) {
for (element in c(2: posicion.pivote-1)) {
cat("\n left element n", element)
cat("\n testing ",v[element])
if (v[element] <= pivote) {
cat("\n",v[element]," va a la izquierda de ",pivote)
izq = c(izq, v[element])
}
}
}
if (posicion.pivote < len) {
for (element in c(posicion.pivote + 1 : len)) {
cat("\n right element n", element)
cat("\n testing ",v[element])
if (v[element] > pivote) {
cat("\n",v[element]," va a la derecha de ",pivote)
der = c(der, v[element])
}
}
}
cat("\n", izq, " | ", der)
cat("\n ----------------------")
if (length(izq) > 1) {
izq <- quicksort(izq)
}
if (length(der) > 1) {
der <- quicksort(der)
}
return(c(izq, pivote, der))
}
v <- c(8, 3, -1, 0, -1, -1)
result <- quicksort(v)
quicksort <- function(v) {
len <- length(v)
posicion.pivote <- sample(c(1: len), 1)
pivote <- v[posicion.pivote]
cat("\n pivote: ", pivote," en posicion: ",posicion.pivote)
izq <- c()
der <- c()
if (posicion.pivote > 1) {
for (element in c(2: posicion.pivote-1)) {
cat("\n left element n", element)
cat("\n testing ",v[element])
if (v[element] <= pivote) {
cat("\n",v[element]," va a la izquierda de ",pivote)
izq = c(izq, v[element])
}
}
}
if (posicion.pivote < len) {
for (element in c(posicion.pivote + 1 : len-1)) {
cat("\n right element n", element)
cat("\n testing ",v[element])
if (v[element] > pivote) {
cat("\n",v[element]," va a la derecha de ",pivote)
der = c(der, v[element])
}
}
}
cat("\n", izq, " | ", der)
cat("\n ----------------------")
if (length(izq) > 1) {
izq <- quicksort(izq)
}
if (length(der) > 1) {
der <- quicksort(der)
}
return(c(izq, pivote, der))
}
v <- c(8, 3, -1, 0, -1, -1)
result <- quicksort(v)
for (element in c(posicion.pivote + 1 : len-2)) {
cat("\n right element n", element)
cat("\n testing ",v[element])
if (v[element] > pivote) {
cat("\n",v[element]," va a la derecha de ",pivote)
der = c(der, v[element])
}
}
quicksort <- function(v) {
len <- length(v)
posicion.pivote <- sample(c(1: len), 1)
pivote <- v[posicion.pivote]
cat("\n pivote: ", pivote," en posicion: ",posicion.pivote)
izq <- c()
der <- c()
if (posicion.pivote > 1) {
for (element in c(2: posicion.pivote-1)) {
cat("\n left element n", element)
cat("\n testing ",v[element])
if (v[element] <= pivote) {
cat("\n",v[element]," va a la izquierda de ",pivote)
izq = c(izq, v[element])
}
}
}
if (posicion.pivote < len) {
for (element in c(posicion.pivote + 1 : len-2)) {
cat("\n right element n", element)
cat("\n testing ",v[element])
if (v[element] > pivote) {
cat("\n",v[element]," va a la derecha de ",pivote)
der = c(der, v[element])
}
}
}
cat("\n", izq, " | ", der)
cat("\n ----------------------")
if (length(izq) > 1) {
izq <- quicksort(izq)
}
if (length(der) > 1) {
der <- quicksort(der)
}
return(c(izq, pivote, der))
}
v <- c(8, 3, -1, 0, -1, -1)
result <- quicksort(v)
7:10
quicksort <- function(v) {
len <- length(v)
posicion.pivote <- sample(c(1: len), 1)
pivote <- v[posicion.pivote]
cat("\n pivote: ", pivote," en posicion: ",posicion.pivote)
izq <- c()
der <- c()
if (posicion.pivote > 1) {
for (element in c(1: posicion.pivote-1)) {
cat("\n left element n", element)
cat("\n testing ",v[element])
if (v[element] <= pivote) {
cat("\n",v[element]," va a la izquierda de ",pivote)
izq = c(izq, v[element])
}
}
}
if (posicion.pivote < len) {
for (element in c(posicion.pivote + 1 : len)) {
cat("\n right element n", element)
cat("\n testing ",v[element])
if (v[element] > pivote) {
cat("\n",v[element]," va a la derecha de ",pivote)
der = c(der, v[element])
}
}
}
cat("\n", izq, " | ", der)
cat("\n ----------------------")
if (length(izq) > 1) {
izq <- quicksort(izq)
}
if (length(der) > 1) {
der <- quicksort(der)
}
return(c(izq, pivote, der))
}
v <- c(8, 3, -1, 0, -1, -1)
result <- quicksort(v)
v <- c(4,5,3,2,6)
result <- quicksort(v)
install.packages("spatstat")
library(spatstat)
data(cells)
knitr::opts_chunk$set(echo = TRUE)
library(spatstat)
library(tidyverse)
data <- read.csv(file="data/Bases_Bicimad.csv")
summary(data)
names(data) <- c("longitude","latitude","object.ID","station","UTM.easting","UTM.northing","district",
"neighborhood","street","number","stands","capacity","stand.type","province.code",
"city.code","postal.code"
)
summary(data)
data<-select(data,-c(capacity,stand.type,province.code,city.code))
summary(data)
#data.subset.1<-select(data, c(longitude,latitude,stands))
data.subset.1<-select(data, c(longitude,latitude))
x.min<-min(data$longitude)
x.max<-max(data$longitude)
y.min<-min(data$latitude)
y.max<-max(data$latitude)
window <- owin(c(x.min, x.max),c(y.min,y.max))
summary(window)
spatial.data<-as.ppp(X=data.subset.1,W=window)
summary(spatial.data)
plot(spatial.data,NULL)
pfit <- ppm(spatial.data, ~latitude)
pfit <- ppm(spatial.data)
plot(pfit)
pfit <- ppm(spatial.data,~1)
plot(pfit)
pfit <- ppm(spatial.data,~longitude)
pfit <- ppm(spatial.data,~spatial.data$longitude)
pfit <- ppm(spatial.data,~spatial.data$longitude)
pfit <- ppm(spatial.data,~y)
plot(pfit)
summary(pfit)
pfit <- ppm(spatial.data,~x+y)
plot(pfit)
summary(pfit)
library("shiny")
library(dplyr)
library(ggplot2)
#Loading the data.
data<-read.csv(file = "../data/clean-twitter-data.csv")
#Creating a column that joins the date and time.
data<-mutate(data, times = paste(Date, Hour))
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
#time.series<-time.series[1:8000,]
setwd("C:/Users/Mariano/Google Drive/eit-health/semestre-1/big-data/practice/big-data-2019/visualization/twitter-time-series")
library("shiny")
library(dplyr)
library(ggplot2)
#Loading the data.
data<-read.csv(file = "../data/clean-twitter-data.csv")
#Creating a column that joins the date and time.
data<-mutate(data, times = paste(Date, Hour))
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
#time.series<-time.series[1:8000,]
times<-(time.series$timestamp)
dates<-as.Date(strptime(times, format="%Y-%m-%d  %H:%M"))
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
# date<-as.Date(timestamp.formatted)
time.series$times<- timestamp.formatted
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = time.series$Date)
times<-(time.series$times)
dates<-as.Date(strptime(times, format="%Y-%m-%d  %H:%M"))
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
# date<-as.Date(timestamp.formatted)
time.series$times<- timestamp.formatted
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = time.series$Date)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = time.series$Date)
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)
runApp()
library("shiny")
library(dplyr)
library(ggplot2)
#Loading the data.
data<-read.csv(file = "../data/clean-twitter-data.csv")
#Creating a column that joins the date and time.
data<-mutate(data, times = paste(Date, Hour))
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)
runApp()
runApp()
setwd("C:/Users/Mariano/Google Drive/eit-health/semestre-1/big-data/practice/big-data-2019/visualization/WordCloudApp")
runApp()
library(tm)
install.packages("wordcloud")
install.packages("wordcloud2")
install.packages("memoise")
install.packages("wordcloud")
install.packages("wordcloud")
runApp()
library(shiny)
library(tm)
library("wordcloud")
library("wordcloud2")
library("memoise")
library(dplyr)
languages = data.frame(
name = c("Spanish",
"English",
"Portuguese",
"Danish",
"French",
"Italian",
"German",
"Turkish",
"Dutch",
"Hungarian",
"Finnish",
"Russian",
"Swedish"),
code = c("es",
"en",
"pt",
"da",
"fr",
"it",
"de",
"tr",
"nl",
"hu",
"fi",
"ru",
"sv"))
#Using "memoise" to automatically cache the results
getTermMatrix <- memoise(function(langName, startDate, endDate) {
lang = languages %>% filter(name == langName)
text <-data %>%
filter(`Tweet language (ISO 639-1)` == lang$code) %>%
filter(Date >= startDate) %>%
filter(Date <= endDate) %>%
select(`Tweet content`)
docs = Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove language common stopwords
docs <- tm_map(docs, removeWords, stopwords(tolower(lang$name)))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("https", "tco", "...", "com"))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
})
runApp()
setwd("C:/Users/Mariano/Google Drive/eit-health/semestre-1/big-data/practice/big-data-2019/visualization/WordCloudApp")
runApp()
library(shiny)
library(tm)
library("wordcloud")
library("wordcloud2")
library("memoise")
library(dplyr)
languages = data.frame(
name = c("Spanish",
"English",
"Portuguese",
"Danish",
"French",
"Italian",
"German",
"Turkish",
"Dutch",
"Hungarian",
"Finnish",
"Russian",
"Swedish"),
code = c("es",
"en",
"pt",
"da",
"fr",
"it",
"de",
"tr",
"nl",
"hu",
"fi",
"ru",
"sv"))
getTermMatrix <- memoise(function(langName, startDate, endDate) {
lang = languages %>% filter(name == langName)
text <-data %>%
filter(`Tweet language (ISO 639-1)` == lang$code) %>%
filter(Date >= startDate) %>%
filter(Date <= endDate) %>%
select(`Tweet content`)
docs = Corpus(VectorSource(text))
toSpace <- content_transformer(function (x , pattern ) gsub(pattern, " ", x))
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
# Convert the text to lower case
docs <- tm_map(docs, content_transformer(tolower))
# Remove numbers
docs <- tm_map(docs, removeNumbers)
# Remove language common stopwords
docs <- tm_map(docs, removeWords, stopwords(tolower(lang$name)))
# Remove punctuations
docs <- tm_map(docs, removePunctuation)
# Eliminate extra white spaces
docs <- tm_map(docs, stripWhitespace)
# Remove your own stop word
# specify your stopwords as a character vector
docs <- tm_map(docs, removeWords, c("https", "tco", "...", "com"))
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
})
View(languages)
data<-read.csv(file = "../data/clean-twitter-data.csv")
runApp()
runApp()
View(data)
runApp('C:/Users/Mariano/Google Drive/eit-health/semestre-1/big-data/practice/big-data-2019/visualization/twitter-time-series')
setwd("C:/Users/Mariano/Google Drive/eit-health/semestre-1/big-data/practice/big-data-2019/visualization/twitter-time-series")
runApp()
runApp()
data<-mutate(data, times = paste(Date, Hour))
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
time.series$Date<- as.Date(time.series$Date)
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)
runApp()
runApp()
runApp()
runApp()
ggplot(data = time.series, aes(x = times, y = freq, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = "Date", y = "Number of tweets", title = "Number of tweets posted at a given time")
runApp()
library(plotly)
install.packages("plotly")
install.packages("plotly")
library("plotly")
runApp()
runApp()
runApp()
View(time.series)
names(time.series) <- c("Time", "Date","Number of tweets")
ggplot(data = time.series, aes(x = Time, y = "Number of tweets", group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = "Time", y = "Number of tweets", title = "Number of tweets posted at a given time")
#Loading the data.
data<-read.csv(file = "../data/clean-twitter-data.csv")
#Creating a column that joins the date and time.
data<-mutate(data, times = paste(Date, Hour))
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
time.series$Date<- as.Date(time.series$Date)
names(time.series) <- c("Time", "Date","Number of tweets")
ggplot(data = time.series, aes(x = Time, y = "Number of tweets", group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = "Time", y = "Number of tweets", title = "Number of tweets posted at a given time")
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
time.series$Date<- as.Date(time.series$Date)
names(time.series) <- c("Time", "Date","Number of tweets")
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = "Time", y = "Number of tweets", title = "Number of tweets posted at a given time")
runApp()
data<-read.csv(file = "../data/clean-twitter-data.csv")
#Creating a column that joins the date and time.
data<-mutate(data, times = paste(Date, Hour))
#Counting the number of tweets per timestamp.
time.series<-plyr::count(select(data, c(times, Date)))
times<-(time.series$times)
time.format<- "%Y-%m-%d %H:%M"
timestamp.formatted <-(as.POSIXct(times, format=time.format))
time.series$times<- timestamp.formatted
time.series$Date<- as.Date(time.series$Date)
names(time.series) <- c("Time", "Date", "Tweets")
ggplot(data = time.series, aes(x = Time, y = Tweets, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = "Time", y = "Number of tweets", title = "Number of tweets posted at a given time")
ggplot(data = time.series, aes(x = Time, y = Tweets, group = 1))+
geom_line(color = "#00AFBB", size = 0.1)+
labs(x = "Time", y = "Number of tweets", title = "Number of tweets posted at a given time")
runApp()
runApp()
